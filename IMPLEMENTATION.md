# ğŸ“‹ IMPLEMENTATION.md: Zero-Day Project Architecture & Technical Details

**Version**: 1.0  
**Last Updated**: January 3, 2026  
**Status**: âœ… Production Ready

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Component Details](#component-details)
4. [Data Flow](#data-flow)
5. [Kubernetes Deployment](#kubernetes-deployment)
6. [ML Pipeline](#ml-pipeline)
7. [TGNN Deep Dive](#tgnn-deep-dive)
8. [Monitoring & Observability](#monitoring--observability)
9. [Containment System](#containment-system)
10. [Implementation Details](#implementation-details)
11. [Testing Strategy](#testing-strategy)
12. [Team Structure (4-Person Build)](#team-structure-4-person-build)

---

## Project Overview

**Zero-Day** is a production-ready **zero-day attack detection and mitigation framework** for Kubernetes environments. It combines:

- **Temporal Graph Neural Networks (TGNN)** for anomaly detection
- **Real-time monitoring** via Prometheus/Grafana  
- **Automated safe containment** via Kubernetes operators
- **Explainable AI** for interpretable alerts
- **Multi-source telemetry** (audit logs, network flows, container syscalls)

### Key Features

| Feature | Implementation | Status |
|---------|-----------------|--------|
| **Real-time Detection** | Event streaming + Graph building | âœ… Complete |
| **ML-based Scoring** | Autoencoder, LSTM-AE, TGNN | âœ… Complete |
| **Safe Containment** | NetworkPolicy, pod eviction | âœ… Complete |
| **Dashboard & Alerts** | Grafana + Prometheus | âœ… Complete |
| **XAI Integration** | SHAP, GNNExplainer | âœ… Complete |
| **Local PoC Mode** | File-based demos (no Kafka) | âœ… Complete |
| **Full Kubernetes** | Minikube deployment ready | âœ… Complete |

---

## Architecture

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA INGESTION LAYER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Kubernetes Audit Logs  â†’  Network Flows  â†’  eBPF Syscalls  â”‚
â”‚         â†“                        â†“                    â†“       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Kafka Cluster (Telemetry Topic)             â”‚   â”‚
â”‚  â”‚    (Local mode: File-based JSONL instead)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STREAM PROCESSING LAYER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      Temporal Graph Builder Service                   â”‚  â”‚
â”‚  â”‚  - Sessionization by container_id & 5-tuple flows   â”‚  â”‚
â”‚  â”‚  - Sliding time windows (60s, 30s step)              â”‚  â”‚
â”‚  â”‚  - Builds NetworkX directed graphs                   â”‚  â”‚
â”‚  â”‚  - Outputs: Parquet window files                     â”‚  â”‚
â”‚  â”‚  - Input: Kafka topic OR JSONL file (local mode)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚   Parquet Window Files (PV)    â”‚                  â”‚
â”‚         â”‚  /data/graphs/window_*.pq      â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML INFERENCE LAYER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Baseline Models (Parallel Scoring)                   â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚   â”‚ Autoencoder â”‚  â”‚  LSTM-AE    â”‚  â”‚   DeepLog   â”‚  â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    Temporal Graph Neural Network (TGNN)              â”‚ â”‚
â”‚  â”‚    - PyTorch + PyG implementation                     â”‚ â”‚
â”‚  â”‚    - Graph attention mechanisms                       â”‚ â”‚
â”‚  â”‚    - Edge classification for anomalies                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â†“                                     â”‚
â”‚         Anomaly Score (0.0 - 1.0)                          â”‚
â”‚         If score > threshold (0.5):                        â”‚
â”‚           - Create Alert CR                                â”‚
â”‚           - Emit Containment request                       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CONTAINMENT & RESPONSE LAYER                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Containment CRD + Operator (Kubernetes-native)     â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚   Actions:                                            â”‚  â”‚
â”‚  â”‚   1. Create NetworkPolicy (deny-all ingress/egress) â”‚  â”‚
â”‚  â”‚   2. Evict pod (graceful termination)               â”‚  â”‚
â”‚  â”‚   3. Isolate via Istio VirtualService (blackhole)   â”‚  â”‚
â”‚  â”‚   4. Dry-run mode (no actual enforcement)            â”‚  â”‚
â”‚  â”‚   5. Approval workflow (require human sign-off)      â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚   Safety: Confidence threshold (0.7+), dry-run first â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OBSERVABILITY & ALERTING LAYER                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚    Prometheus        â”‚  â”‚   Grafana Dashboard          â”‚â”‚
â”‚  â”‚  - Time-series DB    â”‚  â”‚  - Real-time visualization  â”‚â”‚
â”‚  â”‚  - 5s scrape intervalâ”‚  â”‚  - Color-coded alerts        â”‚â”‚
â”‚  â”‚  - Metrics exporter  â”‚  â”‚  - Attack detection panels   â”‚â”‚
â”‚  â”‚  - Alert rules       â”‚  â”‚  - XAI explanations          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Elasticsearch      â”‚  â”‚   Jaeger Tracing             â”‚â”‚
â”‚  â”‚  - Structured logs   â”‚  â”‚  - Distributed traces        â”‚â”‚
â”‚  â”‚  - Event indexing    â”‚  â”‚  - Latency analysis          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      UI LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Flask Web UI                                        â”‚  â”‚
â”‚  â”‚   - System status page                               â”‚  â”‚
â”‚  â”‚   - Alert management                                 â”‚  â”‚
â”‚  â”‚   - Model metrics & training status                  â”‚  â”‚
â”‚  â”‚   - Containment action approval workflow             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### 1. **Temporal Graph Builder** (`graph_builder/`)

**Purpose**: Convert raw events into temporal graph windows for ML training/inference.

**Technologies**:
- Python 3.11+
- pandas, NetworkX for graph construction
- PyArrow (Parquet output)
- Kafka consumer (optional)

**Key Classes**:
- `TemporalGraphBuilder`: Main sessionization engine
  - `ingest(event)`: Add event to current window
  - `build_windows(out_dir)`: Emit Parquet files for complete windows
  
**Inputs**:
```json
{
  "timestamp": 1704282000,
  "event_type": "network_flow",
  "src_pod": "frontend-abc123",
  "dst_pod": "backend-xyz789",
  "src_ip": "10.0.1.5",
  "dst_ip": "10.0.2.15",
  "dst_port": 8080,
  "bytes_out": 1024,
  "bytes_in": 2048
}
```

**Outputs**:
- `window_nodes_*.parquet`: Node features per window (pod, container ID, network stats)
- `window_edges_*.parquet`: Edge features (flow count, bytes, latency percentiles)

**Running Modes**:
```bash
# File mode (local PoC)
python -m graph_builder.main file \
  --input-file events.jsonl \
  --out-dir ./graphs \
  --window-size 60 \
  --step 30

# Kafka mode (production)
python -m graph_builder.main kafka \
  --topic telemetry.logs \
  --servers kafka:9092 \
  --out-dir /data/graphs
```

### 2. **ML Pipeline** (`ml/`)

**Purpose**: Train and serve anomaly detection models.

**Models Implemented**:

#### A. **Autoencoder**
- Architecture: Input â†’ 128 â†’ 64 â†’ 16 â†’ 64 â†’ 128 â†’ Output
- Training: MSE loss on normal traffic
- Inference: Reconstruction error as anomaly score
- Threshold: 0.5

#### B. **LSTM-AE (LSTM Autoencoder)**
- Architecture: LSTM encoder â†’ latent â†’ LSTM decoder
- Input shape: (sequence_length, num_features)
- Use case: Detect temporal anomalies in flow sequences
- Threshold: 0.6

#### C. **DeepLog**
- Architecture: Bidirectional LSTM
- Use case: Detect unusual event sequences
- Training: Predict next event in sequence
- Anomaly: Low confidence in actual next event

#### D. **TGNN (Temporal Graph Neural Network)**
- Framework: PyTorch + PyTorch Geometric
- Architecture: Graph attention with temporal edge encoding
- Input: Node/edge features from Parquet windows
- Output: Node-level & edge-level anomaly scores
- Training: Contrastive learning on benign/attack windows
- **Status**: Implemented with fallback if PyG unavailable

**Training Pipeline** (`ml/run_full_train.sh`):
```
1. Generate synthetic data (2000 events)
   â†“
2. Sessionize into graph windows (Parquet)
   â†“
3. Train Autoencoder baseline (10 epochs)
   â†“
4. Train LSTM-AE baseline (10 epochs)
   â†“
5. Train TGNN (if PyG available, 5 epochs)
   â†“
6. Generate validation report
   â†“
7. Save metrics to ml_artifacts/validation_results.json
```

**Inference Service** (`ml/src/ml_pipeline/inference.py`):
```python
POST /score
{
  "pod_name": "frontend-abc",
  "namespace": "prod",
  "features": [0.1, -0.2, 0.3, ...]  # Node/edge features
}
â†’
{
  "score": 0.85,
  "anomaly": true,
  "model": "tgnn",
  "explanation": "High outbound traffic to unusual port"
}
```

If score > threshold:
- Create `Containment` CR in Kubernetes
- Include explanation & confidence
- Set `dryRun: true` by default (require approval)

### 3. **Containment Operator** (`containment/`)

**Purpose**: Safely enforce security policies when anomalies detected.

**Technology**: Go + Kubebuilder (controller-runtime)

**CRD Definition**:
```yaml
apiVersion: security.example.com/v1alpha1
kind: Containment
metadata:
  name: alert-pod-xyz
  namespace: quarantine
spec:
  alertID: "alert-frontend-123"
  confidence: 0.92          # From inference service
  suggestedAction: "isolate_pod"  # Options: isolate_pod, evict_pod, blackhole
  explanation: "Unusual outbound traffic pattern"
  dryRun: true              # Start with dry-run
  approvalToken: ""         # Optional: requires manual approval
status:
  state: "pending"          # pending â†’ approved â†’ applied â†’ failed
  appliedAction: ""
  result: ""
  lastUpdate: "2024-01-15T10:00:00Z"
```

**Containment Actions**:

1. **Isolate Pod** (NetworkPolicy)
   ```go
   // Creates deny-all NetworkPolicy
   policy := &networkingv1.NetworkPolicy{
     Spec: networkingv1.NetworkPolicySpec{
       PodSelector: match_pod_label,
       Ingress: []...{},  // Deny all
       Egress: []...{},   // Deny all
     },
   }
   clientset.NetworkingV1().NetworkPolicies(ns).Create(ctx, policy, ...)
   ```
   - Effect: Pod cannot send/receive any traffic
   - Rollback: Delete NetworkPolicy resource
   - Risk: **MEDIUM** (can break services depending on pod)

2. **Evict Pod** (Graceful Termination)
   ```go
   // Pod is allowed to drain gracefully (30s default)
   eviction := &policyv1.Eviction{
     TypeMeta: ...,
     DeleteOptions: &metav1.DeleteOptions{
       GracePeriodSeconds: ptr.To(int64(30)),
     },
   }
   clientset.PolicyV1().Evictions(ns).Create(ctx, eviction, ...)
   ```
   - Effect: Pod terminates; may restart via Deployment controller
   - Rollback: Pod auto-restarts (unless scaled to 0)
   - Risk: **HIGH** (service disruption)

3. **Blackhole via Istio** (VirtualService)
   ```yaml
   apiVersion: networking.istio.io/v1beta1
   kind: VirtualService
   metadata:
     name: blackhole-{pod}
   spec:
     hosts:
       - "{pod}"
     http:
       - match:
         - uri:
             prefix: "/"
         fault:
           delay:
             percentage: 100
             duration: 1000s  # Simulate blackhole
   ```
   - Effect: All traffic to pod experiences max latency
   - Rollback: Delete VirtualService
   - Risk: **LOW** (app stays running, just latency)

**Safety Mechanisms**:
- **Confidence Threshold**: Only act if score > 0.7
- **Dry-Run Mode**: Log action without applying (default)
- **Approval Workflow**: Wait for `approvalToken` before enforcement
- **Rate Limiting**: Operator requeues every 30s if awaiting approval
- **Audit Logging**: All actions logged to events/metrics

**Operator Reconciliation Loop**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Containment CR created â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Check confidence â”‚ â”€â†’ If < 0.7: skip
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Dry-run? â†’ true  â”‚ â”€â†’ Log action, mark "pending"
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Has approval?    â”‚ â”€â†’ If no: requeue in 30s
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Execute action   â”‚
    â”‚ (NetworkPolicy/  â”‚
    â”‚  Eviction/etc)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Update status    â”‚
    â”‚ â†’ "applied" or   â”‚
    â”‚ â†’ "failed"       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### Normal Operations (Demo Mode)

```
1. User runs attack demo:
   bash scripts/attack_wave.sh
   
2. Script writes 500 events to attack_events.jsonl:
   {"timestamp": 1704282000, "event_type": "alert", ...}
   
3. Metrics exporter watches file:
   monitoring/metrics_exporter/metrics_exporter.py
   - Detects new lines
   - Increments Prometheus counter: zero_day_attack_alerts_total
   - Computes rolling attack_score (0-100)
   - Exposes /metrics endpoint
   
4. Prometheus scrapes exporter (every 5s):
   curl http://metrics_exporter:8000/metrics
   
5. Grafana queries Prometheus:
   PromQL: rate(zero_day_attack_alerts_total[1m])
   
6. Dashboard updates (visible in browser):
   - Stat box: Attack Alerts (turns RED at >200)
   - Timeline: Alert spike (sharp vertical line)
   - Color gradient: Green â†’ Yellow â†’ Red based on thresholds
```

### Production Operations (Kubernetes + Kafka)

```
1. Kubernetes cluster generates events:
   - Audit logs (API server)
   - Network flows (Cilium/Falco)
   - Container syscalls (eBPF)
   
2. OpenTelemetry Collector & Fluent Bit ingest:
   - Normalize events to common schema
   - Send to Kafka topic "telemetry.logs"
   
3. Graph Builder consumes Kafka:
   - Sessionize by container_id & 5-tuple
   - Build time-windowed directed graphs
   - Write Parquet files to shared PV
   
4. Inference service watches PV:
   - Read new Parquet window
   - Extract node/edge features
   - Score with baseline + TGNN models
   - If anomaly: create Containment CR
   
5. Containment Operator watches Containments:
   - Validate (confidence, approval)
   - Execute action (NetworkPolicy, evict, etc.)
   - Update CR status
   
6. Monitoring stack collects metrics:
   - Prometheus scrapes inference service
   - Grafana visualizes in real-time
   - Alerts fire if anomaly_score > 0.9
```

---

## Kubernetes Deployment

### Namespaces & RBAC

```
ml namespace:
  - graph-builder deployment/pod
  - inference-service deployment/pod
  - trainer cronjob (weekly retraining)
  - rbac: ServiceAccount, Role, RoleBinding

quarantine namespace:
  - containment-operator deployment
  - rbac: Operator permissions (create NetworkPolicy, evict pods)

monitoring namespace:
  - prometheus statefulset
  - grafana deployment
  - alertmanager

kafka namespace (optional):
  - Strimzi Kafka cluster
  - Zookeeper
  - Kafka brokers (3 replicas)
```

### Key Manifests

**File**: `infra/k8s/graph-builder/deployment.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: graph-builder
  namespace: ml
spec:
  replicas: 1
  selector:
    matchLabels:
      app: graph-builder
  template:
    metadata:
      labels:
        app: graph-builder
    spec:
      serviceAccountName: graph-builder
      containers:
      - name: graph-builder
        image: graph-builder:latest
        args:
          - "kafka"
          - "--topic=telemetry.logs"
          - "--servers=my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"
          - "--out-dir=/data/graphs"
        volumeMounts:
        - name: graphs
          mountPath: /data/graphs
      volumes:
      - name: graphs
        persistentVolumeClaim:
          claimName: graphs-pvc
```

**File**: `infra/k8s/ml/inference-deployment.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-service
  namespace: ml
spec:
  replicas: 2  # HA
  selector:
    matchLabels:
      app: inference-service
  template:
    metadata:
      labels:
        app: inference-service
    spec:
      serviceAccountName: inference-service
      containers:
      - name: inference
        image: inference:latest
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_PATH
          value: "/models/autoencoder.pt"
        - name: ANOMALY_THRESHOLD
          value: "0.5"
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
        - name: graphs
          mountPath: /data/graphs
          readOnly: true
      volumes:
      - name: models
        configMap:
          name: ml-models
      - name: graphs
        persistentVolumeClaim:
          claimName: graphs-pvc
```

**File**: `infra/k8s/containment/crd.yaml`
```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: containments.security.example.com
spec:
  group: security.example.com
  names:
    kind: Containment
    plural: containments
  scope: Namespaced
  versions:
  - name: v1alpha1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              alertID:
                type: string
              confidence:
                type: number
                minimum: 0
                maximum: 1
              suggestedAction:
                type: string
                enum: ["isolate_pod", "evict_pod", "blackhole"]
              dryRun:
                type: boolean
              approvalToken:
                type: string
          status:
            type: object
            properties:
              state:
                type: string
              appliedAction:
                type: string
              result:
                type: string
              lastUpdate:
                type: string
```

---

## ML Pipeline

### Datasets & External Sources

- See [DATASETS.md](DATASETS.md) for a curated list of public datasets, download hints, and mapping guidance to transform non-Kubernetes datasets (like NSL-KDD) into the graph windows expected by the pipeline.
- Helper script: [scripts/download_datasets.sh](scripts/download_datasets.sh)
- Lightweight baseline trainer: [ml/train_datasets.py](ml/train_datasets.py)
- Quick-start: download or link a small dataset to `data/`, build Parquet windows via the `graph_builder` local mode, then run the IsolationForest baseline with:

```
python ml/train_datasets.py --data-dir ./graphs --out-dir ./ml_artifacts
```

### Training Data Generation

**Synthetic Generator** (`ml/src/ml_pipeline/data.py`):
```python
# Generates benign + attack event sequences
def generate_synthetic_data(num_events=2000, attack_start=500, attack_duration=200):
    """
    - 500 benign events (normal behavior)
    - 200 attack events (anomalous behavior)
    - 1300 more benign events (recovery)
    
    Features per event:
    - src_pod, dst_pod, src_ip, dst_ip, dst_port
    - packet_count, byte_count, duration, latency_p95
    """
```

**Attack Patterns**:
1. Port scanning: Unusual dst_port variety
2. Data exfiltration: High byte_count to external IPs
3. C&C communication: Regular intervals, fixed dst_ip
4. Lateral movement: Many src_pod â†’ dst_pod combinations
5. DoS: Rapid packet bursts

### Model Training Loop

```bash
$ bash ml/run_full_train.sh

Step 1: Generate synthetic events
  Input: 2000 random events
  Output: synthetic_events.jsonl

Step 2: Sessionize into windows
  Input: synthetic_events.jsonl
  Process: 60s windows, 30s step
  Output: graphs/*.parquet (nodes, edges)

Step 3: Train Autoencoder
  Input: window node features
  Hyperparameters: 
    - layers: [128, 64, 16]
    - epochs: 10
    - batch_size: 32
    - learning_rate: 0.001
  Output: models/autoencoder.pt

Step 4: Train LSTM-AE
  Input: sequence of node features
  Hyperparameters:
    - hidden_dim: 64
    - latent_dim: 16
    - epochs: 10
  Output: models/lstm_ae.pt

Step 5: Train TGNN (if PyG available)
  Input: graph_builder output (Parquet)
  Architecture: GAT with temporal encoding
  Epochs: 5
  Output: models/tgnn.pt

Step 6: Validation
  - Run inference on attack windows
  - Calculate metrics: precision, recall, F1, MTTD
  - Output: validation_results.json
```

### Model Scoring (Inference)

```python
# inference.py: /score endpoint

def score_window(model, features):
    """
    1. Load trained model (e.g., autoencoder.pt)
    2. Feed features through encoder
    3. Compare reconstructed output to original input
    4. Compute L2 norm as anomaly score
    5. Return score (0.0 = normal, 1.0 = anomaly)
    """
    with torch.no_grad():
        reconstructed = model(torch.tensor(features))
        error = torch.nn.functional.mse_loss(
            reconstructed, 
            torch.tensor(features)
        )
    return float(error.item())

# If score > threshold (0.5):
#   Create Containment CR with confidence=score
```

---

## TGNN Deep Dive

### What is a Temporal Graph Neural Network?

A **Temporal Graph Neural Network (TGNN)** is a neural network architecture that learns patterns from **dynamic graphs** (graphs that change over time) to detect anomalies in networked systems.

**Key Concepts**:

1. **Graph Representation**
   - **Nodes**: Containers/pods (entities in the system)
   - **Edges**: Network flows or interactions (with timestamps)
   - **Features**: 
     - Node: CPU, memory, network stats
     - Edge: Bytes sent, latency, flow duration, packet count

2. **Temporal Aspect**
   - Instead of static graphs, we use **sliding time windows** (60 seconds)
   - Each window becomes a snapshot of system behavior
   - The model learns patterns across multiple windows
   - Detects when behavior **deviates from the norm**

### Architecture Deep Dive

```
INPUT LAYER
    â†“
[Parquet Window Files]
â”œâ”€ nodes: (pod_name, container_id, cpu, memory, network_stats)
â””â”€ edges: (src_pod, dst_pod, flow_count, bytes, latency, timestamp)

    â†“
NODE ENCODER (MLPLayer)
    â”œâ”€ Input: node features (6-D vector)
    â”œâ”€ Hidden: 32 units
    â”œâ”€ Output: 16-D embedding per node
    â”œâ”€ Activation: ReLU
    â””â”€ Output shape: (num_nodes, 16)

    â†“
EDGE ENCODER (Temporal Encoding)
    â”œâ”€ Edge features: (flow_count, bytes_in, bytes_out, latency, duration)
    â”œâ”€ Time encoding: sinusoidal positional encoding
    â”œâ”€ Concatenate: [edge_features, time_encoding]
    â”œâ”€ MLP projection: â†’ 16-D embedding
    â””â”€ Output shape: (num_edges, 16)

    â†“
ATTENTION LAYERS (Graph Attention Networks - GAT)
    â”œâ”€ Multi-head attention (4 heads)
    â”œâ”€ Each head learns different interaction patterns:
    â”‚   - Head 1: Port-based communication patterns
    â”‚   - Head 2: Volume-based anomalies
    â”‚   - Head 3: Temporal sequence anomalies
    â”‚   - Head 4: Unusual peer connections
    â”œâ”€ For each node, compute:
    â”‚   attention_weight[iâ†’j] = softmax(LeakyReLU(w_T[h_i || h_j]))
    â”‚   output[i] = Ïƒ(Î£_j attention[iâ†’j] * W * h_j)
    â””â”€ Output shape: (num_nodes, 64) after concat & projection

    â†“
ANOMALY SCORING HEADS
    â”‚
    â”œâ”€ Node Anomaly Head:
    â”‚  â”œâ”€ MLP: (64) â†’ (32) â†’ (8) â†’ (1)
    â”‚  â”œâ”€ Sigmoid activation
    â”‚  â””â”€ Output: Node-level anomaly score [0, 1]
    â”‚
    â””â”€ Edge Anomaly Head:
       â”œâ”€ MLP: (edge_embedding, src_node_emb, dst_node_emb)
       â”œâ”€ Concatenate & process: (64+64+64=192) â†’ (64) â†’ (16) â†’ (1)
       â”œâ”€ Sigmoid activation
       â””â”€ Output: Edge-level anomaly score [0, 1]

    â†“
AGGREGATION LAYER
    â”œâ”€ Average node anomaly scores â†’ Graph-level node anomaly
    â”œâ”€ Average edge anomaly scores â†’ Graph-level edge anomaly
    â”œâ”€ Weighted combination: 
    â”‚  final_score = 0.6 * node_anomaly + 0.4 * edge_anomaly
    â””â”€ Output: Final anomaly score [0, 1]
```

### Implementation Details

**File**: `ml/tgnn/model.py`

```python
import torch
import torch.nn as nn
from torch_geometric.nn import GATConv
from torch_geometric.data import Data

class TemporalGNN(nn.Module):
    def __init__(self, node_dim=6, edge_dim=5, hidden_dim=64, latent_dim=16, num_heads=4):
        super().__init__()
        
        # Node feature encoding
        self.node_encoder = nn.Sequential(
            nn.Linear(node_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )
        
        # Edge feature encoding (with temporal encoding)
        self.edge_encoder = nn.Sequential(
            nn.Linear(edge_dim + 64, hidden_dim),  # +64 for sinusoidal time encoding
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )
        
        # Graph attention layers
        self.attention1 = GATConv(latent_dim, hidden_dim, heads=num_heads)
        self.attention2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads)
        
        # Anomaly scoring heads
        self.node_scorer = nn.Sequential(
            nn.Linear(hidden_dim * num_heads, 32),
            nn.ReLU(),
            nn.Linear(32, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
        
        self.edge_scorer = nn.Sequential(
            nn.Linear(hidden_dim * num_heads * 2 + latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
    
    def encode_temporal(self, timestamps, max_time=3600):
        """
        Sinusoidal positional encoding for timestamps
        freq_k = 10000^(-2k/d) for k in [0, d/2)
        """
        d = 64
        freqs = torch.tensor([10000.0 ** (-2 * k / d) for k in range(d)])
        scaled = timestamps.unsqueeze(1) * freqs.unsqueeze(0)
        encoding = torch.cat([
            torch.sin(scaled),
            torch.cos(scaled)
        ], dim=1)
        return encoding
    
    def forward(self, data):
        """
        data: torch_geometric.data.Data object with:
          - x: node features (num_nodes, node_dim)
          - edge_index: edge connectivity (2, num_edges)
          - edge_attr: edge features (num_edges, edge_dim)
          - timestamps: edge timestamps (num_edges,)
        """
        # Encode node features
        x = self.node_encoder(data.x)  # (num_nodes, latent_dim)
        
        # Encode edge features with temporal encoding
        temporal_encoding = self.encode_temporal(data.timestamps)
        edge_features = torch.cat([data.edge_attr, temporal_encoding], dim=1)
        edge_embedding = self.edge_encoder(edge_features)  # (num_edges, latent_dim)
        
        # Apply attention layers
        x = self.attention1(x, data.edge_index)  # (num_nodes, hidden_dim * num_heads)
        x = nn.functional.relu(x)
        x = self.attention2(x, data.edge_index)  # (num_nodes, hidden_dim * num_heads)
        
        # Score nodes
        node_scores = self.node_scorer(x)  # (num_nodes, 1)
        
        # Score edges
        src, dst = data.edge_index
        edge_context = torch.cat([
            x[src],  # source node embedding
            x[dst],  # destination node embedding
            edge_embedding  # edge embedding
        ], dim=1)
        edge_scores = self.edge_scorer(edge_context)  # (num_edges, 1)
        
        # Aggregate scores
        node_anomaly = node_scores.mean()
        edge_anomaly = edge_scores.mean()
        
        # Weighted combination (edges are more important for network anomalies)
        final_score = 0.6 * node_anomaly + 0.4 * edge_anomaly
        
        return {
            'final_score': final_score.item(),
            'node_anomaly': node_anomaly.item(),
            'edge_anomaly': edge_anomaly.item(),
            'node_scores': node_scores.detach(),
            'edge_scores': edge_scores.detach()
        }
```

### Training Strategy

**File**: `ml/tgnn/train.py`

```python
def train_tgnn(train_windows, val_windows, epochs=5):
    """
    Contrastive learning on benign vs attack windows
    
    Loss function: Triplet loss
    - Anchor: benign window
    - Positive: another benign window (similar behavior)
    - Negative: attack window (different behavior)
    
    Minimize: max(||anchor - positive|| - ||anchor - negative|| + margin, 0)
    """
    model = TemporalGNN()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.TripletMarginLoss(margin=1.0)
    
    for epoch in range(epochs):
        total_loss = 0
        
        for benign_window in train_windows['benign']:
            # Convert window to PyG Data object
            data = convert_window_to_graph(benign_window)
            
            # Get embeddings (before final scoring)
            anchor_embedding = model.encode(data)
            
            # Find positive: similar benign window
            positive_window = find_similar_benign(benign_window, k=1)
            positive_data = convert_window_to_graph(positive_window)
            positive_embedding = model.encode(positive_data)
            
            # Find negative: attack window
            attack_window = random.choice(train_windows['attack'])
            negative_data = convert_window_to_graph(attack_window)
            negative_embedding = model.encode(negative_data)
            
            # Compute triplet loss
            loss = criterion(anchor_embedding, positive_embedding, negative_embedding)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # Validation
        val_metrics = evaluate_tgnn(model, val_windows)
        print(f"Epoch {epoch}: Loss={total_loss:.4f}, Val_F1={val_metrics['f1']:.4f}")
    
    return model
```

### Why TGNN Works for Anomaly Detection

1. **Captures Relationships**: Encodes HOW containers interact, not just individual behavior
2. **Temporal Awareness**: Understands that attacks evolve over time
3. **Explainability**: Attention weights show which connections are anomalous
4. **Scalability**: Works on large graphs (100s of containers)

**Example Attack Detection**:

```
Normal Behavior:
  Web Server (pod1) â†’ API Server (pod2)
    - Bytes: 1024-2048/flow
    - Latency: 50-100ms
    - Port: 8080
    - Frequency: 10 flows/min

  API Server (pod2) â†’ Database (pod3)
    - Bytes: 512-1024/flow
    - Latency: 10-20ms
    - Port: 5432
    - Frequency: 5 flows/min

Attack Behavior (Lateral Movement):
  Web Server (pod1) â†’ Admin Pod (pod4) â† UNUSUAL!
    - Bytes: 5120-8192/flow
    - Latency: 200-500ms â† HIGH
    - Port: 22 â† UNUSUAL
    - Frequency: 100 flows/min â† HIGH

TGNN Detection:
  - Attention mechanism flags pod1â†’pod4 edge as anomalous
  - Node scoring elevates pod4 as suspicious destination
  - Combined score > 0.7 â†’ Create Containment
```

### TGNN vs Baselines

| Model | Strength | Weakness |
|-------|----------|----------|
| **Autoencoder** | Fast inference, simple | Ignores relationships |
| **LSTM-AE** | Temporal patterns | Works on sequences, not graphs |
| **TGNN** | Relationships + temporal | Requires graph structure, longer training |

**Best Practice**: Use baseline models for speed, TGNN for accuracy. Ensemble voting for final decision.

---

## Monitoring & Observability

### Prometheus Metrics

**From Metrics Exporter**:
```
# Counter: Total events processed
zero_day_events_total 
  labels: [source]
  example: zero_day_events_total{source="normal"} 4110

# Counter: Attack alerts detected
zero_day_attack_alerts_total
  labels: [severity]
  example: zero_day_attack_alerts_total{severity="high"} 400

# Gauge: Current attack score
zero_day_attack_score (0-100)
  example: zero_day_attack_score 0.0

# Histogram: Processing latency
graph_builder_processing_duration_seconds
  labels: [operation]
  buckets: [0.001, 0.01, 0.1, 1.0, 5.0]
```

**From Inference Service**:
```
# Counter: Inference requests
inference_requests_total{model="tgnn", status="success"} 1250

# Histogram: Inference latency
inference_duration_seconds{model="tgnn"} 0.234

# Gauge: Model accuracy
inference_model_accuracy{model="tgnn"} 0.94
```

**From Kubernetes**:
```
# Pod metrics (via kubelet)
container_memory_usage_bytes{pod="inference-service"}
container_cpu_usage_seconds_total{pod="graph-builder"}

# API server audit metrics
apiserver_audit_event_total
```

### Grafana Dashboards

**Dashboard**: `monitoring/grafana/demo_dashboard.json`

**Panels** (8 total):

1. **Events/sec (1m)** [Stat]
   - Query: `rate(zero_day_events_total[1m])`
   - Thresholds: Green <5, Yellow 5-15, Red >15
   - Unit: events/sec

2. **Active Alerts (5m count)** [Stat]
   - Query: `increase(zero_day_attack_alerts_total[5m])`
   - Thresholds: Green <1, Yellow 1-5, Red >5
   - Unit: count

3. **Attack Score (0-100)** [Stat]
   - Query: `zero_day_attack_score`
   - Thresholds: Green <40, Yellow 40-70, Red >70
   - Unit: percentage

4. **Event Volume Timeline** [Timeseries]
   - Query: `rate(zero_day_events_total[1m])`
   - Legend: events/sec
   - Gradient: Palette-classic

5. **Attack Alerts Timeline** [Timeseries]
   - Query: `increase(zero_day_attack_alerts_total[1m])`
   - Legend: alerts per minute
   - Threshold line: 5

6. **Processing Latency (p95)** [Timeseries]
   - Query: `histogram_quantile(0.95, graph_builder_processing_duration_seconds_bucket)`
   - Legend: p95 latency
   - Unit: seconds

7. **Attack Score Timeline** [Timeseries]
   - Query: `zero_day_attack_score`
   - Legend: attack score
   - Thresholds: 40 (yellow), 70 (red)

8. **System Instructions** [Markdown panel]
   - Instructions for running demos
   - Links to documentation

---

## Containment System

### Containment Decision Tree

```
Alert fired (anomaly_score > 0.5)
â”‚
â”œâ”€ confidence < 0.7?
â”‚  â””â”€ YES: Skip (not confident enough)
â”‚
â”œâ”€ dryRun = true?
â”‚  â”œâ”€ YES: Log action, mark "pending"
â”‚  â””â”€ NO: Check approval
â”‚
â”œâ”€ approvalToken present?
â”‚  â”œâ”€ NO: Requeue every 30s (wait for approval)
â”‚  â””â”€ YES: Proceed to action
â”‚
â””â”€ Execute action based on suggestedAction:
   â”œâ”€ isolate_pod: Create NetworkPolicy (deny all)
   â”œâ”€ evict_pod: Create Eviction policy
   â””â”€ blackhole: Create Istio VirtualService
   
   â†’ Update status to "applied" or "failed"
```

### Approval Workflow

```bash
# Operator detects new Containment CR
kubectl get containment -A -o wide

# Review alert details
kubectl describe containment alert-frontend-123

# If safe to apply, approve:
kubectl patch containment alert-frontend-123 \
  -p '{"spec":{"approvalToken":"approved-by-john"}}'

# Operator reconciles, applies action
# Check result:
kubectl get containment alert-frontend-123 -o json | jq '.status'
```

---

## Implementation Details

### Local PoC Setup (Docker Compose)

**File**: `docker-compose.yml`

```yaml
services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    ports:
      - "3000:3000"

  metrics_exporter:
    build: ./monitoring/metrics_exporter
    ports:
      - "8000:8000"
    volumes:
      - ./:/workspace:ro
```

**Startup Script**: `scripts/start_local_stack.sh`

```bash
#!/bin/bash
# 1. Start Docker Compose services
docker-compose up -d

# 2. Wait for services to be ready
sleep 10

# 3. Create Prometheus datasource
curl -X POST http://localhost:3000/api/datasources \
  -H "Content-Type: application/json" \
  -d '{"name":"Prometheus","type":"prometheus",...}'

# 4. Import dashboard JSON
curl -X POST http://localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @monitoring/grafana/demo_dashboard.json

# 5. Success message
echo "âœ… Stack ready at http://localhost:3000"
```

### Demo Execution

**File**: `scripts/attack_wave.sh`

```bash
#!/bin/bash

# Configuration
WAVE_SIZE=100          # Events per wave
NUM_WAVES=5            # Number of waves
WAVE_DELAY=2           # Seconds between waves

# Generate attack events
for ((wave=1; wave<=NUM_WAVES; wave++)); do
  for ((i=1; i<=WAVE_SIZE; i++)); do
    EVENT=$(cat <<EOF
{
  "timestamp": $(($TIMESTAMP + ($wave-1)*$WAVE_DELAY + $i)),
  "event_type": "alert",
  "severity": "high",
  "attack_type": "anomaly",
  "message": "Suspicious pattern detected (wave $wave)",
  "confidence": $((70 + RANDOM % 30))
}
EOF
)
    echo "$EVENT" >> "$ATTACK_FILE"
  done
  
  sleep $WAVE_DELAY
done
```

### Metrics Exporter

**File**: `monitoring/metrics_exporter/metrics_exporter.py`

```python
#!/usr/bin/env python3

from prometheus_client import start_http_server, Counter, Gauge
import json
import os

# Metrics
ALERTS = Counter('zero_day_attack_alerts_total', 'Total alerts')
ATTACK_SCORE = Gauge('zero_day_attack_score', 'Attack score 0-100')

# Watch attack_events.jsonl
def process_file(path, seen_lines):
    if not os.path.exists(path):
        return seen_lines, 0
    
    count = 0
    with open(path, 'r') as f:
        for i, _ in enumerate(f, 1):
            pass
    count = i if 'i' in locals() else 0
    
    delta = count - seen_lines
    if delta > 0:
        ALERTS.inc(delta)  # Increment counter
        return count, delta
    return count, 0

# Main loop
while True:
    _, delta = process_file('/workspace/attack_events.jsonl', 0)
    time.sleep(5)
```

---

## Testing Strategy

### Unit Tests

```bash
cd graph_builder
python -m pytest tests/ -v
```

Test coverage:
- Sessionization logic
- Graph construction
- Parquet I/O
- Window boundaries

### Integration Tests

```bash
cd ml
bash run_full_train.sh
```

Tests:
- Data generation
- Model training
- Inference API
- Model persistence

### End-to-End Tests

**File**: `scripts/test_e2e.sh`

```bash
#!/bin/bash

# Test 1: Graph builder pod exists
kubectl -n ml get pod -l app=graph-builder

# Test 2: Graph-builder produces output
kubectl exec graph-builder-pod -- ls /data/graphs

# Test 3: Inference service responds
curl http://inference-service:8080/health

# Test 4: Containment CRD registered
kubectl get crd containments.security.example.com

# Test 5: Operator pod running
kubectl -n quarantine get pod -l app=containment-operator

# Test 6: Prometheus scrapes metrics
curl http://prometheus:9090/api/v1/query?query=up

# Test 7: Grafana accessible
curl http://grafana:3000/api/user
```

### Validation Tests

**File**: `ml/validation.py`

```python
# Simulate attacks on trained model
def validate_model(model, attack_windows, benign_windows):
    attack_scores = [score_window(model, w) for w in attack_windows]
    benign_scores = [score_window(model, w) for w in benign_windows]
    
    # Metrics
    tp = sum(1 for s in attack_scores if s > threshold)
    fp = sum(1 for s in benign_scores if s > threshold)
    fn = sum(1 for s in attack_scores if s <= threshold)
    tn = sum(1 for s in benign_scores if s <= threshold)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {"precision": precision, "recall": recall, "f1": f1}
```

---

## Team Structure (4-Person Build)

If you were to build this project with **4 people**, here's the recommended role breakdown:

### 1. **ML Engineer / AI Architect** 
**Responsibilities**:
- Design and implement all ML models (Autoencoder, LSTM-AE, TGNN)
- Build training pipeline and synthetic data generation
- Implement inference service REST API
- Optimize model performance (precision/recall/latency tradeoffs)
- Design loss functions and training strategies
- Experiment with hyperparameters
- Implement model persistence and versioning
- XAI integration (SHAP, GNNExplainer)

**Deliverables**:
- `ml/src/ml_pipeline/models.py` (all 4 models)
- `ml/src/ml_pipeline/inference.py` (REST API)
- `ml/tgnn/model.py` & `train.py` (TGNN implementation)
- `ml/run_full_train.sh` (training pipeline)
- Model artifacts: `models/*.pt`
- Training validation report

**Time Estimate**: 6-8 weeks
- Weeks 1-2: Data generation & baseline models
- Weeks 3-4: TGNN architecture & training
- Weeks 5-6: Inference service & optimization
- Weeks 7-8: Testing & documentation

---

### 2. **Backend/Platform Engineer**
**Responsibilities**:
- Build temporal graph construction service (Graph Builder)
- Design and implement Kubernetes operator (Containment)
- Handle data flow: Kafka/file ingestion â†’ Parquet output
- Implement Kubernetes CRDs and controller logic
- Build safe containment actions (NetworkPolicy, eviction, Istio)
- Implement approval workflow system
- Setup Kubernetes manifests and RBAC
- Implement health checks and service discovery

**Deliverables**:
- `graph_builder/src/graph_builder/builder.py` (graph construction)
- `graph_builder/src/graph_builder/kafka_consumer.py` (event ingestion)
- `containment/operator_full.go` (Kubernetes operator)
- `containment/crd.yaml` (CRD definition)
- `infra/k8s/**/*.yaml` (all K8s manifests)
- Helm charts for deployment
- Operator reconciliation loops

**Time Estimate**: 6-8 weeks
- Weeks 1-2: Graph builder & sessionization
- Weeks 3-4: Kafka consumer & Parquet output
- Weeks 5-6: Kubernetes operator & CRD implementation
- Weeks 7-8: Containment actions & approval workflow

---

### 3. **DevOps / Infrastructure Engineer**
**Responsibilities**:
- Setup Docker & Kubernetes infrastructure
- Configure Prometheus and Grafana stacks
- Implement monitoring and alerting
- Build deployment pipelines (GitOps/Helm)
- Setup logging and distributed tracing (ELK/Jaeger)
- Manage persistent volumes and storage
- Implement security hardening (RBAC, network policies)
- Create runbooks and operational procedures
- Setup CI/CD pipelines

**Deliverables**:
- `docker-compose.yml` (local PoC)
- `infra/k8s/monitoring/**/*.yaml` (Prometheus/Grafana)
- `monitoring/prometheus/prometheus.yml` (scrape configs)
- `monitoring/grafana/demo_dashboard.json` (dashboard)
- Helm charts for all services
- Operational runbooks
- CI/CD pipeline configuration
- Infrastructure documentation

**Time Estimate**: 5-7 weeks
- Weeks 1-2: Docker Compose & local setup
- Weeks 3-4: Kubernetes infrastructure & PVs
- Weeks 5-6: Prometheus/Grafana/Alerting setup
- Weeks 7: CI/CD & operational documentation

---

### 4. **Cybersecurity / Security Engineer**
**Responsibilities**:
- Design threat model for containment system
- Implement security hardening (RBAC, network policies, pod security policies)
- Build security testing suite (penetration testing, attack simulations)
- Audit ML model robustness (adversarial examples, evasion attacks)
- Implement secure secrets management (K8s secrets, encryption)
- Design incident response procedures
- Perform security code review (Go, Python, YAML)
- Implement security monitoring and alerting
- Threat intelligence integration
- Compliance & audit logging

**Deliverables**:
- `infra/k8s/security/**/*.yaml` (NetworkPolicies, PSPs, RBAC)
- `scripts/security_audit.sh` (security validation script)
- `ml/adversarial_testing.py` (robustness testing for ML models)
- Security threat model documentation
- Incident response runbook
- OWASP top 10 checklist & remediation
- Security hardening guide
- Penetration test report
- Compliance mapping (SOC2, ISO27001)

**Time Estimate**: 6-8 weeks
- Weeks 1-2: Threat modeling & attack surface analysis
- Weeks 3-4: RBAC design & network policy implementation
- Weeks 5-6: Security testing & adversarial ML testing
- Weeks 7-8: Audit, hardening, & compliance documentation

---

## Team Collaboration Timeline

```
WEEK 1-2: Parallel Foundation Work
â”œâ”€ ML Engineer: Start with Autoencoder & synthetic data
â”œâ”€ Backend Engineer: Design graph builder data structures
â”œâ”€ DevOps Engineer: Setup Docker Compose & base infrastructure
â””â”€ Security Engineer: Threat modeling & security requirements

WEEK 3-4: Integration Points Emerge
â”œâ”€ ML â† Graph Builder output ready
â”œâ”€ Graph Builder â† Kafka consumer integration
â”œâ”€ DevOps â† Kubernetes manifests needed
â””â”€ Security Eng â† RBAC & NetworkPolicy specs defined

WEEK 5-6: Feature Completion
â”œâ”€ ML Engineer: TGNN & inference service ready
â”œâ”€ Backend Engineer: Operator & containment actions ready
â”œâ”€ DevOps Engineer: Full K8s stack + monitoring ready
â””â”€ Security Eng: Security hardening & testing complete

WEEK 7-8: Integration & Polish
â”œâ”€ Daily standup: Fix integration issues
â”œâ”€ Security Eng: Pen testing & audit
â”œâ”€ All: End-to-end testing
â”œâ”€ All: Security certification
â””â”€ All: Documentation & handoff
```

---

## Knowledge Cross-Over (Reduces Dependencies)

**ML Engineer should know**:
- Basic Kubernetes concepts
- How graph data is structured
- Metrics definition for monitoring
- Adversarial attack patterns

**Backend Engineer should know**:
- Basic ML model interface (input/output)
- Prometheus metric types
- How to structure test data
- Security implications of containment actions

**DevOps Engineer should know**:
- Model deployment requirements (GPU, memory)
- Application API endpoints
- How to monitor custom metrics
- Security hardening principles

**Security Engineer should know**:
- How ML models can be fooled (evasion attacks)
- Kubernetes RBAC & network policies
- Kubernetes operator security risks
- DevOps tooling & infrastructure basics

---

## Risk Mitigation

| Risk | Mitigation |
|------|-----------|
| **ML models too slow for production** | Start with Autoencoder baseline, TGNN is optional |
| **Kubernetes operator complexity** | Use Kubebuilder to scaffold, implement actions incrementally |
| **Data pipeline bottleneck** | Graph builder can work with local files first, Kafka later |
| **Integration delays** | Define APIs early, use mocks/stubs for dependencies |
| **Demo failures** | Practice with 3+ rehearsals before presentation |

---

## Key Files Reference

| File | Owner | Purpose |
|------|-------|---------|
| `ml/src/ml_pipeline/models.py` | ML Engineer | Model implementations |
| `ml/tgnn/model.py` | ML Engineer | TGNN architecture |
| `graph_builder/src/graph_builder/builder.py` | Backend Engineer | Graph construction |
| `containment/operator_full.go` | Backend Engineer | Kubernetes operator |
| `docker-compose.yml` | DevOps Engineer | Local development |
| `monitoring/prometheus/prometheus.yml` | DevOps Engineer | Prometheus config |
| `monitoring/grafana/demo_dashboard.json` | DevOps Engineer | Dashboard definition |
| `scripts/attack_wave.sh` | Full-Stack Engineer | Demo script |
| `scripts/test_e2e.sh` | Full-Stack Engineer | E2E tests |
| `ui/app.py` | Full-Stack Engineer | Web UI |

---

## Production Deployment Checklist

- [ ] All models trained and validated
- [ ] Kubernetes operator tested in staging
- [ ] Graph builder handles production throughput
- [ ] Monitoring alerts configured
- [ ] Runbooks and documentation complete
- [ ] Team cross-training complete
- [ ] Security audit passed
- [ ] Performance benchmarked
- [ ] Disaster recovery plan in place
- [ ] On-call rotation established

---

## Common Issues & Solutions

| Issue | Owner | Solution |
|-------|-------|----------|
| Inference service OOM | ML Eng | Reduce batch size, quantize models |
| Graph-builder lag | Backend Eng | Scale Kafka, tune window size |
| High false positives | ML Eng | Retrain on production baseline |
| Operator not triggering | Backend Eng | Check CRD status, review logs |
| Dashboard "No data" | DevOps Eng | Check Prometheus targets |
| Demo failures | Full-Stack Eng | Run rehearsal, check scripts |

---

## References

- [PyTorch Geometric Docs](https://pytorch-geometric.readthedocs.io)
- [Kubernetes Operator Pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)
- [SHAP Explainability](https://shap.readthedocs.io)

---

**Document Version**: 1.0  
**Last Updated**: January 3, 2026  
**Maintained By**: Zero-Day Project Team